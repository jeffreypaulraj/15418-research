<!DOCTYPE html>
<html>

<head>
  <!-- Basic -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <!-- Mobile Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <!-- Site Metas -->
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="author" content="" />

  <title>Parallelized Training for  Brain Tumor Classification</title>

  <!-- slider stylesheet -->
  <!-- slider stylesheet -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css" />

  <!-- bootstrap core css -->
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />

  <!-- fonts style -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Poppins:400,700&display=swap" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="css/style.css" rel="stylesheet" />
  <!-- responsive style -->
  <link href="css/responsive.css" rel="stylesheet" />

  <!-- Owl Carousel CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/owl.carousel/dist/assets/owl.carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/owl.carousel/dist/assets/owl.theme.default.min.css">

  <!-- Owl Carousel JS -->
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/owl.carousel/dist/owl.carousel.min.js"></script>

</head>

<body>
  <div class="hero_area">
    <section class=" slider_section position-relative">
      <div class="container">
        <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
          <ol class="carousel-indicators">
            <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
          </ol>
     
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h1>
                        Parallelizing Brain Tumor Image Classification
                      </h1>
                      <h2>
                        Luke Han, Jeffrey Paulraj
                      </h2>
                      <p>
                        15-418: Parallel Computer Architecture and Programming
                      </p>
                      <p>
                        Carnegie Mellon University
                      </p>
                      <div class="">
                        <a href="https://drive.google.com/file/d/1QLKZQRrHZxB8zyn67UI8OUIvbSJVaFGR/view?usp=sharing">
                          Project Proposal
                        </a>
                        <a href="https://drive.google.com/file/d/1T_f_YuQ30ydMaD_KUIz-FRnFR8HoNN5d/view?usp=sharing">
                          Milestone Report
                        </a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
        </div>
      </div>
    </section>
  </div>


  <section class="do_section layout_padding">
    <div class="container">
      <div class="do_container">
        <a href="#background" class="box arrow-start arrow_bg">
          <div class="img-box">
            <img src="images/d-1.png" alt="">
          </div>
          <div class="detail-box">
            <h6>Background</h6>
          </div>
        </a>
        
        <a href="#challenges" class="box arrow-middle arrow_bg">
          <div class="img-box">
            <img src="images/d-1.png" alt="">
          </div>
          <div class="detail-box">
            <h6>Challenges</h6>
          </div>
        </a>
        
        <a href="#resources" class="box arrow-middle arrow_bg">
          <div class="img-box">
            <img src="images/d-1.png" alt="">
          </div>
          <div class="detail-box">
            <h6>Resources</h6>
          </div>
        </a>
        
        <a href="#deliverables" class="box arrow-end arrow_bg">
          <div class="img-box">
            <img src="images/d-1.png" alt="">
          </div>
          <div class="detail-box">
            <h6>Deliverables</h6>
          </div>
        </a>
        
        <a href="#schedule" class="box">
          <div class="img-box">
            <img src="images/d-1.png" alt="">
          </div>
          <div class="detail-box">
            <h6>Schedule</h6>
          </div>
        </a>
      </div>
    </div>
  </section>

  <!-- end do section -->


  <section class="infocontainer">
    <div class="container">
      <div class="row">
        <div class="col-md-5">
          <div class="img-box">
            <img src="images/brain_tumor.png" alt="" style="width: 100%; max-width: 400px; height: auto;">
          </div>
        </div>
        <div class="col-md-7">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
               SUMMARY
              </h2>
            </div>
            <p>
              We will be creating a multi-layered convolutional network to classify brain tumors in MRI scans that is trained via two different approaches: data parallelism and model pipelining parallelism. 
              <br>Our development will focus on the goals of optimizing model accuracy and minimizing training runtime. 
              <br>We will use OpenMP to implement data parallelism training, with gradient updates for different data batches computed separately and communicated across cores.
              <br>We will use MPI to implement model pipelining parallelism, with various cores responsible for processing all data for a specific layer of our model.
            </p>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="infocontainer" id="background">
    <div class="container">
      <div class="row">
        <div class="col-md-7">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                BACKGROUND
              </h2>
            </div>
            <p>
              Our project of Brain Tumor detection from MRI scans is an image classification problem that will significantly benefit from the use of CNNs, which excel at recognizing patterns in visual data. 
              <br>Training a CNN is computationally intensive, particularly for large datasets, due to the repeated calculations involved in gradient descent and backpropagation. Convolutional layers typically consume large portions of compute resources. Fully connected layers, responsible for high-dimensional transformations and final classification, hold most of the model’s parameters and require extensive memory. To address these issues we will be implementing two Convolutional Neural Network models to help us explore the performance and differences between MPI and OpenMP parallelization techniques.
              <br>Gradient Descent is a key optimization technique used to train CNNs, which we will be focusing on as it is inherently time-consuming and resource-intensive due to its iterative nature. Parallelism will help with this as we can distribute its computations.
              <br>When using data parallelism, we can split the dataset across multiple CPU cores allowing for simultaneous gradient computations and faster updates. When using model parallelism, we can divide the model up into multiple nodes, and allow each node to process gradients for a different layer simultaneously as batches of data are sent between layers. This allows for better memory management and helps with the scalability of having large datasets.
            </p>
            
          </div>
        </div>
        <div class="col-md-5">
          <div class="img-box">
            <img src="images/cnn.png" style="width: 100%; max-width: 400px; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="infocontainer" id="challenges">
    <div class="container">
      <div class="row">
        <div class="col-md-5">
          <div class="img-box">
            <img src="images/research_paper_img.png" style="width: 100%; max-width: 400px; height: auto;">
          </div>
        </div>
        <div class="col-md-7">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                THE CHALLENGE
              </h2>
            </div>
            <p>
              Our first implementation will be using data parallelism. We will use OpenMP to divide up the data across our CPU cores. Each core will independently process a portion of the data, and gradient updates will be synchronized across cores to train the model collectively.
              <br>One key challenge with this approach is reducing the bottleneck of computing overall gradient updates across all data points. One common approach is to maintain a “parameter server”, which is responsible for receiving all gradient updates from other cores and accumulating them, but this is not scalable for larger datasets. We aim to use reductions across cores to accumulate updates and minimize bottlenecks.
              <br>Another challenge is evenly distributing data across cores, as some images may take longer to process than others. We will experiment with dynamically assigning batches of data to various cores to make computation times more equitable.
              <br>Our second implementation will use model pipelining parallelism. We will focus on splitting our architecture over multiple nodes, parallelizing the convolution and fully-connected layers by distributing the network’s parameters like weights and biases across nodes using MPI. Each node will handle a segment of the model, computing gradients for its portion of the weights and exchanging updates with other nodes.
              <br>One challenge with pipelining is the high communication overhead between processes, as we will have to send over the processed results of our entire dataset between layers of the model. We plan to limit this by using asynchronous sends to reduce idle time between cores, as well as intensive batching of data so only a limited amount of data is sent at once.
              <br>A second challenge is some layers of the model taking longer time to train, which could result in some cores for faster layers idling. Similar to our first implementation, we will use intensive batching so that layers dependent on slow nodes can start their computations in parallel and minimize idling.
            </p>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="infocontainer" id="resources">
    <div class="container">
      <div class="row">
        <div class="col-md-7">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                RESOURCES
              </h2>
            </div>
            <p>
              <strong>Computing Resources:</strong> Access to multi-core CPU systems that support OpenMP and MPI (through CMU GHC and PSC machines).
              <br><br>
              <strong>Starter Code:</strong> We will implement the CNN and training pipeline from scratch without using high-level libraries (e.g., PyTorch, TensorFlow), ensuring full control over parallelization. We use notes from previous machine learning courses to guide our implementation as well as open-source libraries like OpenCV to process our image data.
              <br><br>
              <strong>Reference Material:</strong> We have been provided with and will use the paper <a href="https://arxiv.org/abs/1412.1189" target="_blank" style="text-decoration: underline;">“One weird trick for parallelizing convolutional neural networks” by Alex Krizhevsky</a>. This paper specifies work on multi-GPU CNN training, particularly the balance between data and model parallelism.
              <br><br>
              <strong>Other Resources Needed:</strong> We are actively considering various brain tumor datasets.
            </p>
          </div>
        </div>
        <div class="col-md-5">
          <div class="img-box">
            <img src="images/brian_tumor_img_2.png" style="width: 100%; max-width: 400px; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="infocontainer" id="deliverables">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                GOALS AND DELIVERABLES
              </h2>
            </div>
  
            <!-- Planned Goals Section -->
            <div class="planned-goals">
              <h3><strong>Planned Goals</strong></h3>
              <ul>
                <li>
                  <strong>Implement a CNN model with OpenMP-based data parallelism for convolutional and fully-connected layers</strong>
                  <br>
                  <span style="text-decoration: underline;"><strong>Objective:</strong></span> The goal is to enable efficient processing of mini-batches across multiple CPU cores using OpenMP. This will involve parallelizing both the convolutional operations and the fully-connected layers of the CNN model. By utilizing OpenMP, we aim to distribute the workload of each layer across available cores, reducing the computation time for each mini-batch.
                  <br>
                  <span style="text-decoration: underline;"><strong>Justification:</strong></span> OpenMP provides an easy-to-use, shared-memory parallelism model that will allow us to optimize performance without extensive modifications to the original codebase. We expect to see a significant reduction in training time for a single machine, especially when processing large mini-batches, which is critical for scaling up CNN training.
                </li>
                <li>
                  <strong>Implement a CNN model with MPI-based model pipelining parallelism for convolutional and fully-connected layers</strong>
                  <br>
                  <span style="text-decoration: underline;"><strong>Objective:</strong></span> This goal involves leveraging MPI (Message Passing Interface) for model parallelism to distribute the convolutional and fully-connected layers' computations across multiple nodes. We will implement a system where the CNN model's weights are partitioned across multiple nodes, and gradient synchronization is performed efficiently during backpropagation.
                  <br>
                  <span style="text-decoration: underline;"><strong>Justification:</strong></span> By introducing MPI-based parallelism, we aim to scale the CNN model to work across a distributed system, which will allow the processing of larger models that do not fit into the memory of a single node. This will also allow the network to process larger datasets, improving both scalability and model performance. We expect to achieve a performance improvement in terms of parallel scalability when comparing this to a single-node, single-threaded approach.
                </li>
                <li>
                  <strong>Achieve noticeable speedup in training time compared to a sequential implementation, while maintaining classification accuracy</strong>
                  <br>
                  <span style="text-decoration: underline;"><strong>Objective:</strong></span> After implementing both data parallelism (OpenMP) and model parallelism (MPI), we aim to demonstrate a significant speedup in training time when compared to the sequential implementation of the CNN model. Importantly, we will ensure that the classification accuracy of the model remains consistent, despite the introduction of parallelism.
                  <br>
                  <span style="text-decoration: underline;"><strong>Justification:</strong></span> Optimizing for parallelism should not come at the cost of accuracy. By focusing on both speedup and accuracy, we will ensure that the model's performance improves not only in terms of efficiency but also in its ability to generalize to new data.
                </li>
              </ul>
            </div>
  
            <div class="stretch-goals" style="overflow-x: auto; white-space: nowrap;">
              <h3><strong>Stretch Goals</strong></h3>
              <div class="stretch-goals-list" style="display: inline-block; width: max-content;">
                <div class="goal">
                  <strong>Emulate the paper by Alex Krizhevsky, creating a model that utilizes both data and model parallelism correctly and efficiently</strong>
                  <br>
                  <span style="text-decoration: underline;"><strong>Objective:</strong></span> This stretch goal involves emulating the architecture described in Alex Krizhevsky’s seminal paper on CNNs, where both data and model parallelism are used in tandem to enhance performance. We will implement a hybrid approach that utilizes OpenMP for data parallelism and MPI for model parallelism, combining both methods to optimize computational efficiency and memory management.
                  <br>
                  <span style="text-decoration: underline;"><strong>Justification:</strong></span> Combining data and model parallelism is a highly challenging task, but if successful, it could lead to substantial improvements in performance, allowing us to scale the CNN model efficiently across multiple machines and CPUs. This would also serve as a validation of the theoretical approach presented in the paper, providing a benchmark for real-world implementations of CNNs at scale.
                </div>
                <div class="goal">
                  <strong>Normalize our data to accommodate MRIs in different format than initial dataset and try to achieve similar accuracy results</strong>
                  <br>
                  <span style="text-decoration: underline;"><strong>Objective:</strong></span> As a part of improving the model's generalization capabilities, we plan to extend the dataset by incorporating additional data points from external sources. This data augmentation strategy will help to improve the model's robustness and ability to generalize to new, unseen data.
                  <br>
                  <span style="text-decoration: underline;"><strong>Justification:</strong></span> By adding new data points, we can assess whether increasing the dataset size leads to a noticeable improvement in classification accuracy, as well as whether the parallelized CNN model can handle this expanded dataset without a loss in performance. This step will also provide valuable insights into the model's scalability with respect to data size.
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- end work section -->
  <section class="Platform Choice" id="platform">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                PLATFORM CHOICE
              </h2>
            </div>
            <p>
              For this data parallelism portion of our project, we will use OpenMP to leverage multi-core CPU capabilities to process data points simultaneously and reduce to a final gradient update. OpenMP’s shared-memory model fits well with the model layers’ need to process mini-batches independently across cores.
              <br>For the model pipelining parallelism portion of our project, we will use MPI to separately evaluate updates for the different layers (convolutional layers, fully connected layers, activation layers) across multiple processes, and create custom messages to communicate the processed data after each stage between processes.
              <br>Our combined choice of OpenMP and MPI will allow us to explore both types of parallelism in-depth, providing a robust and efficient platform for neural network training on CPU clusters. Our primary language of choice will be C++.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="Schedule" id="schedule">
    <div class="container">
      <div class="heading_container">
        <h2>SCHEDULE</h2>
      </div>

      <!-- Owl Carousel -->
      <div class="owl-carousel owl-theme">
        <!-- Week 1 -->
        <div class="item">
          <h3>Week 1 (11/11 - 11/17)</h3>
          <ul>
            <li>Complete Project Proposal</li>
            <li>Obtain the MRI Brain Tumor dataset</li>
            <li>Develop code for various model layers (convolution, activation, fully connected/linear), emphasizing modularity to be used for both data and model parallelism</li>
          </ul>
        </div>

        <!-- Week 2 -->
        <div class="item">
          <h3>Week 2 (11/18 - 11/24)</h3>
          <ul>
            <li>Finish implementing all model layers, create training loop</li>
            <li>Adapt model layers for a specific dataset</li>
            <li>Process image data using OpenCV</li>
            <li>Start implementing data parallelism with OpenMP in the convolution layers distributing
              mini-batches access to our CPU cores</li>
          </ul>
        </div>

        <!-- Week 3 -->
        <div class="item">
          <h3>Week 3 (11/25 - 12/1)</h3>
          <ul>
            <li>Work on implementation of model with data parallelism</li>
          </ul>
        </div>

        <!-- Week 4 -->
        <div class="item">
          <h3>Week 4 (12/2 - 12/8)</h3>
          <ul>
            <li>Test/debug Parallelized forward/back passes on small test samples using OpenMP to verify the correctness of data parallelism</li>
            <li>Complete milestone report on preliminary findings in data parallelism approach</li>
            <li>Start work on implementing model pipelining parallelism, using MPI to pass data between various layers of model</li>
          </ul>
        </div>

        <!-- Week 5 -->
        <div class="item">
          <h3>Week 5 (12/9-12/13) - Project Poster Session</h3>
          <ul>
            <li>Complete Model Parallelism approach</li>
            <li>Fine-tune hyperparameters for both approaches, optimize for prediction accuracy</li>
            <li>Measure training runtime for both approaches under various parameters (batch sizes, activations, etc.)</li>
            <li>Write final report</li>
            <li> Create research poster to convey findings</li>
          </ul>
        </div>
      </div>
    </div>
  </section>
  
  
  <!-- Milestone Report Section -->
  <section class="Milestone Report" id="milestone-report">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                MILESTONE REPORT
              </h2>
            </div>

            <!-- Progress Subsection -->
            <div class="subsection" id="progress">
              <h3>Progress</h3>
              <p>
                Over the past few weeks, we have made significant progress in creating the model layers for our CNN,
                including the convolution layer, linear layer, activation layer, max-pooling layer, and loss-computing
                layer. We have also made our model configurable to use multiple convolution layers, based on time
                and accuracy metrics. However, since we are creating all of these model layers from scratch, this
                development took longer than expected, and consumed much of week 2.
                <br><br>
                This led us to delay the development of our first parallelism approach: data parallelism.
                However, development of this model has begun and we are currently in the debugging/testing stage.
                We have revised our schedule as reflected below. We plan on completing our data parallelism model
                and starting the pipelining parallelism model in Week 4, then concluding all development and our
                final paper in Week 5.
                <br><br>
                However, we are very confident we will have all of our deliverables completed by Week 5.
                We have noticed during our development that there are many reusable components across models,
                which will allow for much faster development for the pipelining model.
              </p>
            </div>

            <!-- Final Deliverables Subsection -->
            <div class="subsection" id="final-deliverables">
              <h3>Final Deliverables</h3>
              <p>
                In our final presentation, we aim to show an overview of our two models (data parallelism and
                pipelining parallelism), and show multiple graphs to compare their classification accuracy and
                training time against the baseline sequential model across multiple epochs of training. We also plan
                on tuning various hyperparameters, such as batch size and learning rate, and presenting graphs on our
                findings of the effects of this tuning.
              </p>
            </div>

            <!-- Concerns Subsection -->
            <div class="subsection" id="concerns">
              <h3>Concerns</h3>
              <p>
                We do not have any significant concerns at this time, as we have already planned how we will be
                implementing our second model, including our parallelism approach and a division of work among
                ourselves. Since we will be reusing components from our first model, we anticipate developing our
                second model will be much faster.
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  

  <section class="infocontainer" id="approach">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                APPROACH
              </h2>
            </div>
  
            <!-- Implementation Section -->
            <div class = "subsection" id="implementation">
              <h3>Implementation</h3>
              <p>
                Our implementation focuses on creating five different Convolutional Neural Networks (CNN) and corresponding training loops using three parallelism strategies: sequential execution (no parallelism), data parallelism, and model pipelining parallelism. For the latter two strategies, we create two different models each: a 1D CNN with six layers, and a 2D CNN with nine layers. Each approach has been designed to use multi-core CPUs while addressing synchronization and workload distribution challenges.
              </p>
              <p>
                Our baseline approach, which was the sequential execution, runs the entire model on a single process without parallelization. It serves as a reference for performance comparisons.
              </p>
            </div>
  
            <!-- Technologies Utilized Section -->
            <div class= "subsection", id = "technologies-utilized">
              <h3> Technologies Utilized</h3>

              <p>
                Our initial implementation utilized C++; however, due to some library constraints and development flexibility, we switched over to Python to write all our convolutional neural network (CNN) functions. We purposely avoided using Python libraries that would simplify the creation of this neural network, such as PyTorch or TensorFlow, and instead wrote our own implementations of the core PyTorch-like APIs. This decision allowed us to have full control over the implementation details of both the model itself and the corresponding training loop while gaining a deeper understanding of the underlying operations involved in a neural network.
              </p>
              <p>
                The primary libraries we used were NumPy, which provided efficient array operations and mathematical computations, and Python's multiprocessing library for managing processes on multiple cores, synchronization primitives (such as locks and thread-safe queues), and shared memory. We specifically targeted a multi-core CPU for process-based parallelism, as it allowed us to take advantage of multiple cores to distribute the workload of convolution operations and other key computations.
              </p>
            </div>
  
            <!-- Problem Mapping to Parallel Machines Section -->
            <div class= "subsection", id = "problem-mapping">
              <h3>Problem Mapping to Parallel Machines</h3>
             
              <p>
                For Data Parallelism, each worker (process) has its own copy of the model in local memory, as well as access to a shared model in the parameter server, and works on a different subset of the data. Synchronization is achieved using locks to ensure each worker can calculate weight updates with its given input batch to the shared model without overwriting effects from other batches. This shared model then propagates these new weights to each worker's local model at the start of each epoch. We wait for all processes to finish before moving to the next epoch. This approach maximizes compute usage but requires efficient synchronization for gradient exchange.
              </p>
              <p>
                For Pipelining Parallelism, the model is split into multiple stages, where each stage is assigned to a different process. Input data flows through the pipeline in a staggered manner using mini-batches, improving utilization of compute resources. We use thread-safe queues to communicate data between the layers on different workers, which have their own memory space. We store the model in shared memory so all processes can make gradient updates to different parts of the same model.
              </p>
            </div>
  
            <!-- Changes to the Original Serial Algorithm Section -->
            <div class="subsection", id = "changes-to-algorithm">
              <h3>Changes to the Original Serial Algorithm</h3>
              <p>
                We made significant changes to our original sequential algorithm to better map onto a parallel machine. In the sequential implementation, all operations such as convolution, activation, pooling, flattening, and fully connected linear layers are executed one after the other for each batch of input data. This approach is inherently limited by the sequential nature of the computation, which underutilizes parallel hardware resources. To address this, two types of parallelism were introduced: data parallelism and model (pipeline) parallelism.
              </p>
              <p>
                For data parallelism, the same operation is applied concurrently on multiple processes for different data batches. To implement this, we had to rewrite our training loop to spawn multiple processes on each epoch, and create a new initial model on each process in their private memory spaces. We also initialized the parameter server in a shared memory region, with a concurrency lock to ensure correctness of gradient updates. We also had to create new methods to override weights on a model.
              </p>
              <p>
                For pipeline parallelism, we rewrote the entire model architecture as well as the training loop. Rather than computing forward/backward passes for all layers in one function, we spawned two new processes for each layer, for its forward and backward passes respectively. We also created one thread-safe queue for each process spawned. We add the input image data for each batch into the queue for the forward pass of the first layer in our model, which will perform the forward operation then pass the output into the queue for the second layer. Once all forward passes for the batch are complete, the loss layer will send its total loss to the backward pass of the last layer, which updates its weights by calculating gradients and passing these gradients to the previous layer's backward pass.
              </p>
              <p>
                Data flows through the pipeline in batches, enabling concurrent computation across multiple stages. These changes effectively reduce idle time, improve resource utilization, and take advantage of the parallel processing capabilities of modern hardware, leading to better scalability and faster execution compared to the original sequential version.
              </p>
            </div>
  
            <!-- Original Code Section -->
            <div class="subsection", id ="original-code">
              <h3>Original code</h3>
              <p>
                We used a basic reference for the individual layers for our sequential models from previous ML courses we took; however, all the layers we implemented for both Data and Model parallelism approaches were made on our own, as well as all the modules and helper functions created from scratch.
              </p>
            </div>
  
          </div>
        </div>
      </div>
    </div>
  </section>
  







  <!-- Results Section -->
<section class="Results" id="results">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <div class="detail-box">
          <div class="heading_container">
            <h2>
              RESULTS
            </h2>
          </div>

          <!-- Performance Speedup -->
          <div class="subsection" id="performance-speedup">
            <h3>Performance Speedup</h3>
            <p>
              We defined our performance in terms of the speedup of the Data and Model Parallelism approaches compared to
              our benchmark sequential approach, measured in seconds. The dataset consisted of around 4000 images, and we trained
              each model (1D and 2D Convolutional Neural Network) for 50 epochs, tracking both idle time and total time required for
              each model run.
            </p>
            <!-- Insert Image for Speedup Analysis -->
            <img src="images/table.png" alt="Speedup and Accuracy Comparison" width="1100" height="500"/>
            <p>
              As seen in the table above, the speed of Data Parallelism was significantly better than Model Parallelism when compared
              to our benchmark sequential approach. We believe this is due to increased communication overheads and idle time from
              computation delays in different layers. Data Parallelism excelled in efficiency, achieving faster training times due to its
              ability to dynamically allocate computational resources based on individual batch processing times, minimizing idle time
              and balancing the workload across processors.
            </p>
          </div>

          <!-- Accuracy Comparison -->
          <div class="subsection" id="accuracy-comparison">
            <h3>Accuracy Comparison</h3>
            <p>
              As seen in the tables, both the pipeline and data parallelism models achieved slightly lower accuracy than the sequential
              model. Since the sequence of gradient updates is more randomized, the magnitude of gradient momentum decreases, resulting
              in smaller gradient updates per epoch and lower overall accuracy. We also observed a substantial difference in the accuracy
              comparisons between the 1D and 2D convolutions. As the number of layers in the neural network increased, overfitting of the
              data seemed to occur.
            </p>
          </div>

          <!-- Batch Size Impact -->
          <div class="subsection" id="batch-size-impact">
            <h3>Impact of Batch Size</h3>
            <!-- Insert Image for Batch Size Impact -->
            <img src="images/1d_time 5.02.16 PM.png"  width="500" height="400"/>
            <img src="images/2d_time 5.02.16 PM.png" width="500" height="400" />
            <p>
              We noticed that the performance of the Data Parallelism model using a smaller batch size was much slower compared to using
              a batch size of 256. With a larger batch size, there is less overhead per batch. This includes the time spent on loading
              data, setting up computations, and managing synchronization across threads or processing units. Larger batch sizes help
              amortize these overheads, reducing the relative time spent on setup and communication.
            </p>
          </div>

          <!-- Loss Metrics Analysis -->
          <div class="subsection" id="loss-metrics">
            <h3>Loss Metrics</h3>
            <img src="images/1d_loss 5.02.16 PM.png" width="500" height="400" />
            <img src="images/2d_loss 5.02.16 PM.png" width="500" height="400" />
            <p>
              Another key aspect we noticed in our tests was the loss metrics for each model. For Data Parallelism, we observed large
              fluctuations in the loss metrics, which became increasingly volatile as the epochs progressed. We believe this was due
              to the lack of a scheduler to adjust the learning rates during later epochs, leading to instability in training.
            </p>
          </div>

          <!-- Analysis of Speedup Limitations -->
          <div class="subsection" id="speedup-limitations">
            <h3>Analysis of Speedup Limitations</h3>
            <img src="images/idle_time_comparison.png" width="600" height="600" />

            <p>
              The primary factor limiting speedup in our implementation was synchronization overhead and the resulting idle times in
              the threads. As the batch size increased, the time spent on updating weights grew longer, causing each thread to hold onto
              resources (such as locks or shared variables) for a longer duration. This caused idle times for other threads, as they
              waited for the thread with the longest update time to finish. Consequently, the parallelism was not fully utilized, and
              the expected speedup was not achieved.
            </p>
            <p>
              Although the system theoretically supported parallelism, the need for synchronization in the weight update step
              introduced a bottleneck, especially with larger models or weights. This limited the scalability of the parallelism and
              caused significant idle time in the threads, reducing overall efficiency. Future optimizations could include reducing
              synchronization frequency, increasing the work per thread (e.g., processing larger batches of data per thread), or exploring
              asynchronous updates to allow threads to continue working without waiting for others. These optimizations could reduce idle
              times and improve the utilization of available parallel resources, ultimately improving performance.
            </p>
          </div>

          <!-- GPU Performance Analysis -->
          <div class="subsection" id="gpu-performance">
            <h3>GPU Performance Considerations</h3>
            <p>
              We believe that model parallelism would have performed better on the GPU due to its ability to efficiently manage
              large-scale parallel computations. GPUs leverage high throughput and memory bandwidth to handle distributed model stages
              more effectively, potentially offering better scalability and performance for our model parallelism approach.
            </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>




  <!-- info section -->
  <section class="info_section ">
    <div class="container">
      <div class="row">
        <div class="col-md-3">
          <div class="info_contact">
            <h5>
              About
            </h5>
            <div>
              <div class="img-box">
                <img src="images/envelope-white.png" width="18px" alt="">
              </div>
              <p>
                lhhan@andrew.cmu.edu
              </p>
            </div>
            <div>
              <div class="img-box">
                <img src="images/envelope-white.png" width="18px" alt="">
              </div>
              <p>
                jpaulraj@andrew.cmu.edu
              </p>
            </div>
          </div>
        </div>

        <!-- <div class="col-md-3">
          <div class="info_insta">
            <h5>
              Instagram
            </h5>
            <div class="insta_container">
              <div>
                <a href="">
                  <div class="insta-box b-1">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
                <a href="">
                  <div class="insta-box b-2">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
              </div>

              <div>
                <a href="">
                  <div class="insta-box b-3">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
                <a href="">
                  <div class="insta-box b-4">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
              </div>
              <div>
                <a href="">
                  <div class="insta-box b-3">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
                <a href="">
                  <div class="insta-box b-4">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="col-md-3">
          <div class="info_form ">
            <h5>
              Newsletter
            </h5>
            <form action="">
              <input type="email" placeholder="Enter your email">
              <button>
                Subscribe
              </button>
            </form>
            <div class="social_box">
              <a href="">
                <img src="images/fb.png" alt="">
              </a>
              <a href="">
                <img src="images/twitter.png" alt="">
              </a>
              <a href="">
                <img src="images/linkedin.png" alt="">
              </a>
              <a href="">
                <img src="images/youtube.png" alt="">
              </a>
            </div>
          </div>
        </div> -->
      </div>
    </div>
  </section>

  <!-- end info_section -->


  <!-- footer section -->
  <section class="container-fluid footer_section">
    <p>
      &copy; 2024 All Rights Reserved; Website Template from 
      <a href="https://html.design/">Free Html Templates</a>
    </p>
  </section>
  <!-- footer section -->

  <script type="text/javascript" src="js/jquery-3.4.1.min.js"></script>
  <script type="text/javascript" src="js/bootstrap.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js">
  </script>
  <!-- owl carousel script 
    -->
  <script type="text/javascript">
    $(".owl-carousel").owlCarousel({
      loop: true,
      margin: 0,
      navText: [],
      center: true,
      autoplay: true,
      autoplayHoverPause: true,
      responsive: {
        0: {
          items: 1
        },
        1000: {
          items: 3
        }
      }
    });
  </script>
  <!-- end owl carousel script -->

</body>

</html>